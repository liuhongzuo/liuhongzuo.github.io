#pragma once
#include "core/framework/execution_provider.h"

namespace onnxruntime {

// 继承 IExecutionProvider
class VirtualExecutionProvider : public IExecutionProvider {
 public:
  explicit VirtualExecutionProvider(const VirtualExecutionProviderInfo& info)
      : IExecutionProvider{onnxruntime::kVirtualExecutionProvider} { // 定义你的 EP 类型名称
    
    // 初始化你的 Device 分配器（如果是真实硬件，这里要分配 Device 内存）
    // 这里我们只是模拟，复用 CPU 分配器
    AllocatorCreationInfo default_memory_info(
        [](int) { return std::make_unique<CPUAllocator>(); },
        0, true);
    InsertAllocator(CreateAllocator(default_memory_info));
  }

  // 核心函数 1: 告诉 ORT 你支持哪些节点
  std::vector<std::unique_ptr<ComputeCapability>> 
  GetCapability(const onnxruntime::GraphViewer& graph_viewer,
                const IKernelLookup& /*kernel_lookup*/) const override;

  // 核心函数 2: 提供你的 Kernel 实现注册表
  std::shared_ptr<KernelRegistry> GetKernelRegistry() const override;
};
} // namespace onnxruntime

#include "virtual_execution_provider.h"

namespace onnxruntime {

std::vector<std::unique_ptr<ComputeCapability>> 
VirtualExecutionProvider::GetCapability(const onnxruntime::GraphViewer& graph_viewer,
                                        const IKernelLookup& /*kernel_lookup*/) const {
  std::vector<std::unique_ptr<ComputeCapability>> result;

  // 遍历图中所有节点
  for (const auto& node : graph_viewer.Nodes()) {
    // 假设我们的 VirtualEP 只支持 "Add" 算子
    if (node.OpType() == "Add") {
      
      // 创建一个 ComputeCapability
      std::unique_ptr<ComputeCapability> cc = std::make_unique<ComputeCapability>();
      
      // 定义这是一个单一节点执行 (如果是编译器后端，可能会把多个节点融合成一个子图)
      cc->sub_graph = nullptr; 
      cc->nodes_to_optimize.push_back(node.Index());
      
      // 声明这个能力属于 VirtualEP
      cc->type = "VirtualExecutionProvider";
      
      result.push_back(std::move(cc));
    }
  }
  return result;
}
}


#include "core/framework/op_kernel.h"

namespace onnxruntime {

class VirtualAdd : public OpKernel {
 public:
  VirtualAdd(const OpKernelInfo& info) : OpKernel(info) {}

  Status Compute(OpKernelContext* context) const override {
    // 1. 获取输入 Tensor
    const auto* X = context->Input<Tensor>(0);
    const auto* Y = context->Input<Tensor>(1);
    
    // 2. 准备输出 Tensor
    auto X_shape = X->Shape();
    auto* Z = context->Output(0, X_shape);

    // 3. 执行计算 (这里为了演示，直接用 CPU 循环模拟硬件加速)
    const float* x_data = X->Data<float>();
    const float* y_data = Y->Data<float>();
    float* z_data = Z->MutableData<float>();
    
    size_t size = X_shape.Size();
    for (size_t i = 0; i < size; i++) {
        // 假装我们在 Virtual NPU 上执行
        z_data[i] = x_data[i] + y_data[i]; 
    }

    return Status::OK();
  }
};

// 定义 Kernel 注册表
ONNX_OPERATOR_KERNEL_EX(
    Add,                                    // Op Name
    kOnnxDomain,                            // Domain
    7,                                      // Version
    kVirtualExecutionProvider,              // Provider Type
    KernelDefBuilder()
        .TypeConstraint("T", DataTypeImpl::GetTensorType<float>()), 
    VirtualAdd);                            // Kernel Class
}


std::shared_ptr<KernelRegistry> VirtualExecutionProvider::GetKernelRegistry() const {
  static std::shared_ptr<KernelRegistry> kernel_registry = std::make_shared<KernelRegistry>();
  // 这里的 BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(..., VirtualAdd)> 是宏展开后的辅助函数
  // 实际开发中，通常会有一个注册辅助函数来自动处理所有 Kernels
  return kernel_registry;
}



